---
title: "drake - leaps and bounds in high-performance computing"
slug: "drake-hpc"
date: 2018-05-18
package_version: 5.1.3.9001
authors:
  - name: Will Landau
    url: https://github.com/wlandau
categories:
  - technotes
tags:
- r
- software
- package
- reproducibility
- high-performance-computing
- pipeline
- workflow
---

The [`drake` R package](https://github.com/ropensci/drake) is not only a reproducible research solution, but also a serious high-performance computing engine. The [Get Started page](https://ropensci.github.io/drake/articles/drake.html) introduces [`drake`](https://github.com/ropensci/drake), and this technical note draws from the guides on [high-performance computing](https://ropensci.github.io/drake/articles/parallelism.html) and [timing](https://ropensci.github.io/drake/articles/timing.html).

### You can help!

Some of these features are brand new, and others are newly refactored. The [GitHub version](https://github.com/ropensci/drake) (5.1.3.9001) has all the advertised functionality, but the CRAN version (5.1.2) is behind. If you use [`drake`](https://github.com/ropensci/drake) for your own work, please consider supporting development by field-testing the claims below and posting feedback [here](https://github.com/ropensci/drake/issues/369).

### Let `drake` schedule your targets.

A typical workflow is a sequence of interdependent data transformations. Consider the example from the [Get Started page](https://ropensci.github.io/drake/articles/drake.html).

<iframe
src = "/img/blog-images/2018-05-18-drake-hpc/dag.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

Here, `drake` processes `"raw_data.xlsx"`, `raw_data`, and then `data` in sequence. Once `data` completes, `fit` and `hist` can launch in parallel, and then  `"report.md"` knits once everything else is done. It is `drake`'s responsibility to deduce this order of execution, hunt for ways to parallelize your work, and free you up to focus on the substance of your research.

### Activate parallel processing.

Simply set the `jobs` argument to an integer greater than 1. The following `make()` recruits multiple processes on your local machine.

```r
make(plan, jobs = 2)
```

For parallel deployment to a computing cluster ([SLURM](https://slurm.schedmd.com/), [TORQUE](http://www.adaptivecomputing.com/products/open-source/torque/), [SGE](http://www.univa.com/products/), etc.) `drake` recruits packages [`future`](https://github.com/HenrikBengtsson/future), [`batchtools`](https://github.com/mllg/batchtools), and [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools). First, create a [`batchtools` template file](https://github.com/mllg/batchtools/tree/master/inst/templates) to declare your resource requirements and [environment modules](http://modules.sourceforge.net/). [`Drake`](https://github.com/ropensci/drake) has built-in example files, but you will likely need to tweak yours by hand.

```r
drake_batchtools_tmpl_file("slurm") # Writes batchtools.slurm.tmpl.
```

Next, tell [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) to talk to the cluster.

```r
library(future.batchtools)
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
```

Finally, set `make()`'s `parallelism` argument equal to `"future"` or `"future_lapply"`.

```r
make(plan, parallelism = "future", jobs = 8)
```

### Choose a scheduling algorithm.

The `parallelism` argument to `make()` controls not only where to deploy the workers, but also how to schedule them.

|                       | Deploy: local | Deploy: remote |
| --------------------- |:-------------:| -----:|
| <b>Schedule: persistent</b> | "mclapply", "parLapply" | "future_lapply" |
| <b>Schedule: transient</b>  | | "future", "Makefile" |
| <b>Schedule: staged</b>     | "mclapply_staged", "parLapply_staged" | |

In persistent scheduling, `make(jobs = 2)` deploys three processes: two workers and one master. Whenever a worker is idle, the master assigns it the next target whose dependencies are fully ready. The workers keep running until no more targets remain.

<center>
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"></iframe>
<br>
</center>

If the time limits of your cluster are too strict, consider transient scheduling instead. Here, `make()` starts a brand new worker for each individual target.

<div style = "text-align: center">
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/cHIdOqudELB/" frameborder="0"></iframe>
<br>
</div>

Persistent and transient scheduling are new additions to `drake`. The original algorithm is staged scheduling, in which there is no master process and the workers run in lockstep.<sup><a href="#note1" id="note1ref">[1]</a></sup>

<div style = "text-align: center">
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/dQKbGttIYud/" frameborder="0"></iframe>
<br>
</div>

We lose parallel efficiency, but we also lose overhead. E.g. for [this flood of tiny conditionally independent targets](https://ropensci.github.io/drake/articles/parallelism.html#staged-scheduling), staged scheduling blasts through the [perfectly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) work faster than the alternatives. Consider it if your dependency graph is tall and thin.

<div style = "text-align: center">
<iframe
src = "/img/blog-images/2018-05-18-drake-hpc/staged.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</div>

### How many `jobs` should you choose?

Functions `predict_runtime()` and `predict_load_balancing()` provide guidance. Let's revisit the [`mtcars` example](https://ropensci.github.io/drake/articles/example-mtcars.html).

<div style = "text-align: center">
<iframe
src = "/img/blog-images/2018-05-18-drake-hpc/mtcars.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</div>

Plan for non-staged scheduling, assume every formal non-file target (black circle) takes 2 hours to build, and rest assured that everything else is super quick. If you declare runtime assumptions with the `known_times` argument and cycle over a reasonable range of `jobs`, `predict_runtime()` paints a clear picture.

<div style = "text-align: center">
<img src="/img/blog-images/2018-05-18-drake/times.png" style = "width: 300px" alt="times">
</div>

Your choice of `jobs` for `make()` ultimately depends on the runtime you can tolerate and the computing resources at your disposal. If you rely on a 4-core laptop, maybe call `make(jobs = 4)` in the evening and let your project run overnight. But armed with several vacant nodes on a powerful cluster, you might spin up 14 jobs and then head out for a long lunch.

Contrary to intuition, there is a potential 2-hour speed improvement from 6 to 7 jobs, but not from 7 to 8. Wondering why? It is a fun puzzle to work out yourself, and the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) has a major hint.

### Thanks!

When I attended [`RStudio::conf(2018)`](https://www.rstudio.com/conference/), [`drake`](https://github.com/ropensci/drake) relied almost exclusively on staged scheduling.<sup><a href="#note2" id="note2ref">[2]</a></sup> [Kirill MÃ¼ller](https://github.com/krlmlr) spent hours on site and hours afterwards coaching me, strategizing, and pointing out tools such as [priority queues](https://en.wikipedia.org/wiki/Priority_queue), [message queues](https://en.wikipedia.org/wiki/Message_queue), and the [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). His generous help was essential to [`drake`](https://github.com/ropensci/drake)'s latest enhancements.

### Footnotes

<a id="note1" href="#note1ref">[1]</a> The video of staged scheduling is an oversimplification. It holds mostly true for `make(parallelism = "parLapply_staged")`, but `make(parallelism = "mclapply_staged")` is a bit different. In the former case, each stage is a call to `parLapply()`, which recycles existing workers on a pre-built parallel socket (PSOCK) cluster. But in the latter, every stage is a new call to `mclapply()` that launches a brand new batch of workers. In that sense, the workers in `make(parallelism = "parLapply_staged")` are sort of persistent, and the workers in `make(parallelism = "mclapply_staged")` are sort of transient for some projects.

<a id="note2" href="#note2ref">[2]</a> At the time, the only non-staged backend was `make(parallelism = "Makefile")`.

### Disclaimer

This post is a product of my own personal experience and opinions and does not necessarily represent the official views of my employer. The embedded videos were created using [PowToon](https://www.powtoon.com) and used only as explicitly permitted in the [Terms and Conditions of Use](https://www.powtoon.com/terms-and-conditions/). I make no copyright claim on any of the constituent graphics.
