---
title: "drake - improvements in high-performance computing"
slug: "drake-hpc"
date: 2018-05-25
package_version: 5.1.3.9001
authors:
  - name: Will Landau
    url: https://github.com/wlandau
categories:
  - technotes
tags:
- r
- software
- package
- reproducibility
- high-performance-computing
- pipeline
- workflow
---

The [`drake` R package](https://github.com/ropensci/drake) is not only a reproducible research solution, but also a serious high-performance computing engine. The [Get Started page](https://ropensci.github.io/drake/articles/drake.html) covers the prerequisite concepts, and this technical note touches on material from the guides on [high-performance computing](https://ropensci.github.io/drake/articles/parallelism.html) and [timing](https://ropensci.github.io/drake/articles/timing.html).

### You can help!

Some of these features are brand new, and others are newly refactored. The the [GitHub version](https://github.com/ropensci/drake) (5.1.3.9001) is up to date, but the CRAN version is behind (5.1.2). If you use [`drake`](https://github.com/ropensci/drake) for your own work, please consider test driving these features and posting feedback [here](https://github.com/ropensci/drake/issues/369). It would greatly help development.

### Let `drake` do the scheduling.

A typical workflow is a sequence of data transformations, and those transformations must happen in the correct order. In the example from the [Get Started page](https://ropensci.github.io/drake/articles/drake.html) page, `drake` needs to finish `data` before it starts `fit` or `hist`.

<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/dag.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

So `drake` processes `raw_data.xlsx`, then `raw_data`, and then `data`, all in sequence. Only then does it turn to `fit` and `hist`, in parallel if applicable, moving on to `"report.md"` only after both have completed. All this planning is automatic. Just declare each target in isolation and let `drake` worry about how all the pieces fit together. No matter what high-performance computing configuration you choose, your custom code is free to focus on the intellectual content of your research.

### Activate parallel processing.

Just set the `jobs` argument to a number greater than 1. The following `make()` recruits multiple processes on your local machine.

```r
make(plan, jobs = 2)
```

And thanks to packages [`future`](https://github.com/HenrikBengtsson/future), [`batchtools`](https://github.com/mllg/batchtools), and [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools), it is straightforward to deploy these jobs to a computing cluster. First, create a [`batchtools` template file](https://github.com/mllg/batchtools/tree/master/inst/templates) to declare the appropriate resources and [environment modules](http://modules.sourceforge.net/).

```r
drake_batchtools_tmpl_file("slurm") # Writes batchtools.slurm.tmpl.
```

Next, tell [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) to talk to the cluster.

```r
library(future.batchtools)
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
```

Finally, set `make()`'s `parallelism` argument equal to `"future"` or `"future_lapply"`.

```r
make(plan, parallelism = "future", jobs = 2)
```

### Choose a scheduling algorithm.

The `parallelism` argument to `make()` controls not only where to deploy workers, but also how to coordinate them.

|                       | Deploy: local | Deploy: remote |
| --------------------- |:-------------:| -----:|
| <b>Schedule: persistent</b> | "mclapply", "parLapply" | "future_lapply" |
| <b>Schedule: transient</b>  | | "future", "Makefile" |
| <b>Schedule: staged</b>     | "mclapply_staged", "parLapply_staged" | |

In persistent scheduling, `make(jobs = 2)` deploys three processes: two workers and one master. Whenever a worker is idle, the master assigns it the next target whose dependencies have are ready. The workers keep running until there are no more targets left in the queue.

<center>
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"></iframe>
<br>
</center>

But some clusters, especially at universities, impose strict time limits on jobs. So in transient scheduling, `make()` spends a little extra overhead to create a brand new worker for each target.

<center>
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/cHIdOqudELB/" frameborder="0"></iframe>
<br>
</center>

Persistent and transient scheduling are new in `drake`. The original algorithm was staged scheduling, in which there is no master process and the workers run in lockstep.<sup><a href="#note1" id="note1ref">[1]</a></sup>

<center>
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/dQKbGttIYud/" frameborder="0"></iframe>
<br>
</center>

We lose parallel efficiency, but we also lose overhead. For examples like [this one](https://ropensci.github.io/drake/articles/parallelism.html#staged-scheduling), where most of the targets are conditionally independent, the effect can be net positive. If your workflow graph is extremely tall, consider staged scheduling.

<center>
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/staged.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</center>

### How many `jobs` should you choose?

As the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) explains, functions `predict_runtime()` and `predict_load_balancing()` provide guidance. Let's take the [`mtcars` example](https://ropensci.github.io/drake/articles/example-mtcars.html) with non-staged scheduling, and let's assume every formal target except the R Markdown report (each black circle) takes 2 hours to build and everything else is quick.

<center>
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/mtcars.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</center>

Your choice for `make()`'s `jobs` argument ultimately depends on the runtime you are willing to tolerate and the amount of computing resources you have. If you are using a 4-core laptop, maybe call `make(jobs = 4)` and let the computation run overnight. On a serious computing cluster with multiple nodes, you might consider 7 or 14 jobs.

Contrary to intuition, there is a potential speed improvement from 6 to 7 jobs, but not from 7 to 8. Wondering why? See the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) for a spoiler.

<center>
<!--img src="/img/blog-images/2018-02-06-drake/times.png" style = "width: 300px" alt="times-->
</center>

### Thanks

When I attended [`RStudio::conf(2018)`](https://www.rstudio.com/conference/), [`drake`](https://github.com/ropensci/drake) relied almost exclusively on staged scheduling.<sup><a href="#note2" id="note2ref">[2]</a></sup> [Kirill MÃ¼ller](https://github.com/krlmlr) spent hours on-site and hours afterwards coaching me, strategizing, and pointing to essentials such as [priority queues](https://en.wikipedia.org/wiki/Priority_queue), [message queues](https://en.wikipedia.org/wiki/Message_queue), and the [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). His help was essential for this new feature set.

### Footnotes

<a id="note1" href="#note1ref">[1]</a> The video of staged parallelism is an oversimplification. It holds mostly true for `make(parallelism = "parLapply_staged")`, but `make(parallelism = "mclapply_staged")` is a bit different. In the former case, each stage is a call to `parLapply()`, which recycles existing workers on a pre-built parallel socket (PSOCK) cluster. But in the latter, every stage is a new call to `mclapply()`, which launches a brand new batch of workers. In that sense, workers in `make(parallelism = "parLapply_staged")` are sort of persistent, and workers in `make(parallelism = "mclapply_staged")` are sort of transient for some projects.

<a id="note2" href="#note2ref">[2]</a> At the time, the only non-staged backend was `make(parallelism = "Makefile")`.

### Disclaimer

This post is a product of my own personal opinions and does not necessarily represent the official views of my employer.
