---
title: "drake - improvements in high-performance computing"
slug: "drake-hpc"
date: 2018-05-25
package_version: 5.1.3.9001
authors:
  - name: Will Landau
    url: https://github.com/wlandau
categories:
  - technotes
tags:
- r
- software
- package
- reproducibility
- high-performance-computing
- pipeline
- workflow
---

The [`drake` R package](https://github.com/ropensci/drake) is not only a reproducible research solution, but also a serious high-performance computing engine. The [Get Started page](https://ropensci.github.io/drake/articles/drake.html) covers the prerequisite concepts, and this technical note touches on material from the guides on [high-performance computing](https://ropensci.github.io/drake/articles/parallelism.html) and [timing](https://ropensci.github.io/drake/articles/timing.html).

### You can help!

Some of these features are brand new, and others are newly refactored. The the [GitHub version](https://github.com/ropensci/drake) (5.1.3.9001) has the changes, but the CRAN version is behind (5.1.2). If you use [`drake`](https://github.com/ropensci/drake) for your own work, please consider supporting development by test driving the refurbished high-performance computing functionality and posting feedback [here](https://github.com/ropensci/drake/issues/369).

### Let `drake` schedule the targets.

A typical workflow is a sequence of interdependent data transformations. Consider the example from the [Get Started page](https://ropensci.github.io/drake/articles/drake.html) page.

<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/dag.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

Here,`drake` processes `raw_data.xlsx`, `raw_data`, and `data` one after the other. Once `data` completes, `fit` and `hist` can run in parallel, and then  `"report.md"` begins to knit when everything else is done. `Drake` figures all this out no matter how you order the rows of your `drake_plan()`. It is `drake`'s responsibility to deduce execution order, hunt for ways to parallelize your work, and free up you and your custom code to focus on the real substance of your research.

### Activate parallel processing.

Simply set the `jobs` argument to an integer greater than 1. The following `make()` recruits multiple processes on your local machine.

```r
make(plan, jobs = 2)
```

To build targets on a computing cluster, `drake` can make use of packages [`future`](https://github.com/HenrikBengtsson/future), [`batchtools`](https://github.com/mllg/batchtools), and [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools). First, create a [`batchtools` template file](https://github.com/mllg/batchtools/tree/master/inst/templates) to declare the appropriate resources and [environment modules](http://modules.sourceforge.net/).

```r
drake_batchtools_tmpl_file("slurm") # Writes batchtools.slurm.tmpl.
```

Next, tell [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) to talk to the cluster.

```r
library(future.batchtools)
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
```

Finally, set `make()`'s `parallelism` argument equal to `"future"` or `"future_lapply"`.

```r
make(plan, parallelism = "future", jobs = 2)
```

### Choose a scheduling algorithm.

The `parallelism` argument to `make()` controls not only where to deploy the workers, but also how to schedule them.

|                       | Deploy: local | Deploy: remote |
| --------------------- |:-------------:| -----:|
| <b>Schedule: persistent</b> | "mclapply", "parLapply" | "future_lapply" |
| <b>Schedule: transient</b>  | | "future", "Makefile" |
| <b>Schedule: staged</b>     | "mclapply_staged", "parLapply_staged" | |

In persistent scheduling, `make(jobs = 2)` deploys three processes: two workers and one master. Whenever a worker is idle, the master assigns it the next target whose dependencies are all ready. The workers keep running until there are no more targets left downstream.

<center>
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"></iframe>
<br>
</center>

If you are using a cluster with a strict time limits, consider transient scheduling instead. Here, `make()` creates a brand new worker for each individual target.

<div style = "text-align: center">
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/cHIdOqudELB/" frameborder="0"></iframe>
<br>
</div>

Persistent and transient scheduling are new to `drake`. The original algorithm is staged scheduling, in which there is no master process and the workers run in lockstep.<sup><a href="#note1" id="note1ref">[1]</a></sup>

<div style = "text-align: center">
<br>
<iframe width="700" height="434" src="https://www.powtoon.com/embed/dQKbGttIYud/" frameborder="0"></iframe>
<br>
</div>

We lose parallel efficiency, but we also lose overhead. For examples like [this one](https://ropensci.github.io/drake/articles/parallelism.html#staged-scheduling), where most of the targets are conditionally independent, the effect can be net positive. So if your workflow graph is extremely tall, consider staged scheduling.

<div style = "text-align: center">
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/staged.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</div>

### How many `jobs` should you choose?

As the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) explains, functions `predict_runtime()` and `predict_load_balancing()` provide guidance. Let's take the [`mtcars` example](https://ropensci.github.io/drake/articles/example-mtcars.html) and assume non-staged scheduling. Also suppose every formal non-file target (black circle) takes 2 hours to build and everything else is quick.

<div style = "text-align: center">
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/mtcars.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</div>

Your choice of `jobs` for `make()` ultimately depends on the runtime you are willing to tolerate and the amount of computing resources you have. If you are using a 4-core laptop, maybe call `make(jobs = 4)` and let the computation run overnight. On a serious computing cluster with multiple nodes, you might consider 7 or 14 jobs.

Contrary to intuition, there is a potential speed improvement from 6 to 7 jobs, but not from 7 to 8. Wondering why? See the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) for the spoiler.

<div style = "text-align: center">
<img src="/img/blog-images/2018-02-06-drake/times.png" style = "width: 300px" alt="times>
</div>

### Thanks

When I attended [`RStudio::conf(2018)`](https://www.rstudio.com/conference/), [`drake`](https://github.com/ropensci/drake) relied almost exclusively on staged scheduling.<sup><a href="#note2" id="note2ref">[2]</a></sup> [Kirill MÃ¼ller](https://github.com/krlmlr) spent hours on-site and hours afterwards coaching me, strategizing, and pointing out tools such as [priority queues](https://en.wikipedia.org/wiki/Priority_queue), [message queues](https://en.wikipedia.org/wiki/Message_queue), and the [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). Kirill was essential to  [`drake`](https://github.com/ropensci/drake)'s latest advancements in high-performance computing.

### Footnotes

<a id="note1" href="#note1ref">[1]</a> The video of staged parallelism is an oversimplification. It holds mostly true for `make(parallelism = "parLapply_staged")`, but `make(parallelism = "mclapply_staged")` is a bit different. In the former case, each stage is a call to `parLapply()`, which recycles existing workers on a pre-built parallel socket (PSOCK) cluster. But in the latter, every stage is a new call to `mclapply()` that launches a brand new batch of workers. In that sense, workers in `make(parallelism = "parLapply_staged")` are sort of persistent, and workers in `make(parallelism = "mclapply_staged")` are sort of transient for some projects.

<a id="note2" href="#note2ref">[2]</a> At the time, the only non-staged backend was `make(parallelism = "Makefile")`.

### Disclaimer

This post is a product of my own personal opinions and does not necessarily represent the official views of my employer.
