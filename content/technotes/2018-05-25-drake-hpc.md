---
title: "drake - improvements in high-performance computing"
slug: "drake-hpc"
date: 2018-05-25
package_version: 5.1.3.9001
authors:
  - name: Will Landau
    url: https://github.com/wlandau
categories:
  - technotes
tags:
- r
- software
- package
- reproducibility
- high-performance-computing
- pipeline
- workflow
---

```r
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE
)
```

The [`drake` R package](https://github.com/ropensci/drake) is not only a reproducible research solution, but also a serious high-performance computing engine. If you have not already done so, check out the improved [Get Started page](https://ropensci.github.io/drake/articles/drake.html). The technical notes here cover material from the guides on [high-performance computing](https://ropensci.github.io/drake/articles/parallelism.html) and [timing](https://ropensci.github.io/drake/articles/timing.html).

### You can help!

Some of these features are brand new, and others are newly refactored. The changes are in the [GitHub version](https://github.com/ropensci/drake) but not yet on CRAN. If you use [`drake`](https://github.com/ropensci/drake) for your own work, it would greatly help development to try out these features and post feedback [here](https://github.com/ropensci/drake/issues/369).

### Recruit parallel workers.

Just set the `jobs` argument to activate parallel processing. The following `make()` recruits multiple processes on your local machine.

```r
make(plan, jobs = 2)
```

And thanks to packages [`future`](https://github.com/HenrikBengtsson/future), [`batchtools`](https://github.com/mllg/batchtools), and [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools), it is staightforward to deploy these jobs to a computing cluster. First, create a [`batchtools` template file](https://github.com/mllg/batchtools/tree/master/inst/templates) to declare the appropriate resources and [environment modules](http://modules.sourceforge.net/).

```r
drake_batchtools_tmpl_file("slurm") # Writes batchtools.slurm.tmpl.
```

Tell [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) to talk to the cluster.

```r
library(future.batchtools)
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
```

And finally, set the `parallelism` argument to `"future"` or `"future_lapply"` when you call `make()`.

```r
make(plan, parallelism = "future", jobs = 2)
```

### Let `drake` figure out when to run things.

A typical workflow is a sequence of data transformations, and those transformations must happen in the correct order. In the example from the [Get Started page](https://ropensci.github.io/drake/articles/drake.html) page, `drake` needs to finish `data` before it starts `fit` or `hist`.

<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/dag.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

So `drake` processes `raw_data.xlsx`, then `raw_data`, and then `data`, all in sequence. Only then does it turn to `fit` and `hist` in parallel, moving on to `"report.md"` only after both have completed. All this planning is automatic. Just declare each target in isolation and let `drake` worry about how all the pieces fit together.

### Scheduling algorithms

The `parallelism` argument to `make()` controls not only where to deploy workers, but also and how to coordinate them.

|                       | Deploy: local | Deploy: remote |
| --------------------- |:-------------:| -----:|
| <b>Schedule: persistent</b> | "mclapply", "parLapply" | "future_lapply" |
| <b>Schedule: transient</b>  | | "future", "Makefile" |
| <b>Schedule: staged</b>     | "mclapply_staged", "parLapply_staged" | |

In persistent scheduling, `make(jobs = 2)` deploys three processes: two workers and one master. Whenever a worker is idle, the master assigns it the next target whose dependencies have are ready. The workers keep running until there are no more targets left in the queue.

<center>
<br>
<iframe width="640" height="360" src="https://www.powtoon.com/embed/fsHPN0DTn5i/" frameborder="0"></iframe>
<br>
</center>

But some clusters, especially at universities, impose strict time limits on jobs. So in transient scheduling, `make()` spends a little extra overhead to create a brand new worker for each target.

<center>
<br>
<iframe width="640" height="360" src="https://www.powtoon.com/embed/dPTubZwZqtx/" frameborder="0"></iframe>
<br>
</center>

Persistent and transient scheduling are new in `drake`. The original algorithm was staged scheduling, in which there is no master process and the workers run in lockstep.<sup><a href="#note1" id="note1ref">[1]</a></sup>

<center>
<br>
<iframe width="640" height="360" src="https://www.powtoon.com/embed/fkwJXrmh0hz/" frameborder="0"></iframe>
<br>
</center>

We lose parallel efficiency, but we also lose overhead. For examples like [this one](https://ropensci.github.io/drake/articles/parallelism.html#staged-scheduling), where most of the targets are conditionally independent, the effect can be net positive. If your workflow graph is extremely tall, consider staged scheduling.

<center>
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/staged.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</center>

### How many `jobs` do you need?

As the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) explains, functions `predict_runtime()` and `predict_load_balancing()` provide guidance. Let's take the [`mtcars` example](https://ropensci.github.io/drake/articles/example-mtcars.html) with non-staged scheduling, and let's assume every formal target except the R Markdown report (each black circle) takes 2 hours to build and everything else is quick.

<center>
<iframe
src = "/img/blog-images/2018-02-25-drake-hpc/mtcars.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>
</center>

Contrary to intuition, there is a potential speed improvement from 6 to 7 jobs, but not from 7 to 8. Wondering why? See the [timing guide](https://ropensci.github.io/drake/articles/timing.html#strategize-your-high-performance-computing) for a spoiler.

<center>
<img src="/img/blog-images/2018-02-06-drake/times.png" style = "width: 300px" alt="times>
</center>

### Thanks

When I attended [`RStudio::conf(2018)`](https://www.rstudio.com/conference/), [`drake`](https://github.com/ropensci/drake) relied almost exclusively on staged scheduling.<sup><a href="#note2" id="note2ref">[2]</a></sup> [Kirill MÃ¼ller](https://github.com/krlmlr) spent hours on-site and hours afterwards coaching me, strategizing, and pointing to essentials such as [priority queues](https://en.wikipedia.org/wiki/Priority_queue), [message queues](https://en.wikipedia.org/wiki/Message_queue), and the [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). I feel lucky that he educated me and spurred me to action.

### Footnotes

<a id="note1" href="#note1ref">[1]</a> The video of staged parallelism is an oversimplification. It holds mostly true for `make(parallelism = "parLapply_staged")`, but `make(parallelism = "mclapply_staged")` is a bit different. In the former case, each stage is a call to `parLapply()`, which recycles existing workers on a pre-built parallel socket (PSOCK) cluster. But in the latter, every stage is a new call to `mclapply()`, which launches a brand new batch of workers. In that sense, workers in `make(parallelism = "parLapply_staged")` are sort of persistent, and workers in `make(parallelism = "mclapply_staged")` are sort of transient for some projects.

<a id="note2" href="#note2ref">[2]</a> At the time, the only non-staged backend was `make(parallelism = "Makefile")`.

### Disclaimer

This post reflects my own personal opinions and does not necessarily represent the official views of my employer.
