---
title: tokenizers tutorial
package_version: 0.1.4
---

```{r, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(
  fig.path = "../assets/tutorial-images/tokenizers/",
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE
)
```

In natural language processing, tokenization is the process of breaking human-readable text into machine readable components. The most obvious way to tokenize a text is to split the text into words. But there are many other ways to tokenize a text, the most useful of which are provided by this package.

The tokenizers in this package have a consistent interface. They all take either a character vector of any length, or a list where each element is a character vector of length one. The idea is that each element comprises a text. Then each function returns a list with the same length as the input vector, where each element in the list contains the tokens generated by the function. If the input character vector or list is named, then the names are preserved, so that the names can serve as identifiers.

<section id="installation">

## Installation

```{r eval=FALSE}
install.packages("tokenizers")
```

Or development version from GitHub

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("ropensci/tokenizers")
```

```{r}
library("tokenizers")
```

<section id="usage">

## Usage


### Examples of tokenizers

Using the following sample text, the rest of this vignette demonstrates the different kinds of tokenizers in this package.

```{r}
library(tokenizers)
options(max.print = 25)

james <- paste0(
  "The question thus becomes a verbal one\n",
  "again; and our knowledge of all these early stages of thought and feeling\n",
  "is in any case so conjectural and imperfect that farther discussion would\n",
  "not be worth while.\n",
  "\n",
  "Religion, therefore, as I now ask you arbitrarily to take it, shall mean\n",
  "for us _the feelings, acts, and experiences of individual men in their\n",
  "solitude, so far as they apprehend themselves to stand in relation to\n",
  "whatever they may consider the divine_. Since the relation may be either\n",
  "moral, physical, or ritual, it is evident that out of religion in the\n",
  "sense in which we take it, theologies, philosophies, and ecclesiastical\n",
  "organizations may secondarily grow.\n"
)
```

#### Characters

```{r}
tokenize_characters(james)[[1]]
```

You can also tokenize character-based shingles.

```{r}
tokenize_character_shingles(james, strip_non_alphanum = FALSE)[[1]][1:20]
```


#### Words and word stems

```{r}
tokenize_words(james)
```

Word stemming is provided by the [SnowballC](https://cran.r-project.org/package=SnowballC) package.

```{r}
tokenize_word_stems(james)
```

You can also provide a vector of stop words. A default is provided for several languages by the `stopwords()` function.

```{r}
tokenize_words(james, stopwords = stopwords())
```

#### Sentences and paragraphs

```{r}
tokenize_sentences(james)
tokenize_paragraphs(james)
```

#### N-grams and skip n-grams

```{r}
tokenize_ngrams(james, n = 5, n_min = 2)
tokenize_skip_ngrams(james, n = 5, k = 2)
```

#### Miscellaneous

```{r}
tokenize_lines(james)
tokenize_regex(james, pattern = "[,.;]")
```

<section id="citing">

## Citing

To cite `tokenizers` in publications use:

<br>

> Lincoln Mullen (2016). tokenizers: A Consistent Interface to Tokenize
  Natural Language Text. R package version 0.1.4. https://cran.rstudio.com/package=tokenizers

<section id="license_bugs">

## License and bugs

* License: [MIT](http://opensource.org/licenses/MIT)
* Report bugs at [our Github repo for tokenizers](https://github.com/ropensci/tokenizers/issues?state=open)

[Back to top](#top)
